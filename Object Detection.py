# -*- coding: utf-8 -*-
"""Copy of project 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HZoTZFigio0Ygu_zWMHkJQK12XdYNySk
"""

!mkdir Train1             # dir making
!wget www-old.emt.tugraz.at/~pinz/data/GRAZ_02/bike.zip --no-check-certificate          # graz dataset
!unzip bike.zip -d Train1
!wget www-old.emt.tugraz.at/~pinz/data/GRAZ_02/cars.zip --no-check-certificate
!unzip cars.zip -d Train1
!wget www-old.emt.tugraz.at/~pinz/data/GRAZ_02/none.zip --no-check-certificate
!unzip none.zip -d Train1
!wget www-old.emt.tugraz.at/~pinz/data/GRAZ_02/person.zip --no-check-certificate
!unzip person.zip -d Train1

!rm -rf Train1/person/.ipynb_checkpoints        # remove person checkpnts
!rm -rf Train1/none/.ipynb_checkpoints
!rm Train1/none/Thumbs.db                         # thumbs of none removed
!rm Train1/person/Thumbs.db

from tensorflow.keras.preprocessing.image import img_to_array, load_img          # keras used for pre-prcessing nd data augmentation
import cv2                         # computer vision lib
import os                           # imports path in operating system
datadir = 'Train1/'
data = []
labels = []
dirs = os.listdir(datadir)  
a = ''
for dl in dirs:
  d = os.path.join(datadir, dl)
  a = d
  b = os.listdir(a)
  print(dl) 
  for img in b:
    igd = os.path.join(d,img)
    labels.append(dl)
    ig = load_img(igd)
    arr = img_to_array(ig)  
    arr = cv2.resize(arr, (224,224)) 
    data.append(arr)

data[0].shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D,Flatten,Dense,BatchNormalization, Dropout, Activation
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array
from keras.utils import np_utils
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.models import Model
from matplotlib import pyplot as plt
import numpy as np
import random
import tensorflow as tf
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.optimizers import Adam

cnn4 = Sequential()
cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='elu', input_shape=(224,224,3)))
cnn4.add(BatchNormalization())

cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='elu'))
cnn4.add(BatchNormalization())
cnn4.add(MaxPool2D(pool_size=(2, 2)))
cnn4.add(Dropout(0.25))

cnn4.add(Conv2D(64, kernel_size=(3, 3), activation='elu'))
cnn4.add(BatchNormalization())
cnn4.add(Dropout(0.25))

cnn4.add(Conv2D(128, kernel_size=(3, 3), activation='elu'))
cnn4.add(BatchNormalization())
cnn4.add(MaxPool2D(pool_size=(2, 2)))
cnn4.add(Dropout(0.25))

cnn4.add(Conv2D(128, kernel_size=(3, 3), activation='elu'))
cnn4.add(BatchNormalization())
cnn4.add(MaxPool2D(pool_size=(2, 2)))
cnn4.add(Dropout(0.25))

cnn4.add(Flatten())

cnn4.add(Dense(512, activation='elu'))
cnn4.add(BatchNormalization())
cnn4.add(Dropout(0.5))

cnn4.add(Dense(128, activation='elu'))
cnn4.add(BatchNormalization())
cnn4.add(Dropout(0.5))

cnn4.add(Dense(4, activation='softmax'))

cnn4.compile(loss=categorical_crossentropy,
              optimizer=Adam(),
metrics=['accuracy'])

data = np.array(data)
data = data.astype('float32')
data/=255
lc = LabelEncoder()
label = lc.fit_transform(labels)
labels = np_utils.to_categorical(label,4)

X_train, X_test, Y_train, Y_test = train_test_split(data,labels,test_size=0.10 , random_state=10 )

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
from tensorflow.keras.callbacks import TensorBoard
import datetime
#tensorboard = TensorBoard(log_dir='log/{}'.format("graph"))
log_dir="logs/fit/"
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, \
    height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\
    horizontal_flip=True, fill_mode="nearest")
# H = cnn4.fit_generator(aug.flow(data, labels, batch_size=32), \
#     validation_data=(X_test,Y_test), \
#     steps_per_epoch=len(data), epochs=5, callbacks = [tensorboard] , verbose=1,)
cnn4.fit(data,labels,batch_size=16,validation_data=(X_test,Y_test),epochs=15,verbose=1, callbacks=[tensorboard_callback])

#%tensorboard --logdir logs/fit/

score = cnn4.evaluate(X_train,Y_train, verbose=1)
score

cnn4.summary()

import tensorflow as tf

import tensorflow_hub as hub              

# For downloading the image.
import matplotlib.pyplot as plt
import tempfile            #creating nd handling temporary files
from six.moves.urllib.request import urlopen                  #url lib. request and url open 
from six import BytesIO                    #file object for binary data

# For drawing onto the image.
import numpy as np
from PIL import Image                         #PIL  python interpreter for image editting
from PIL import ImageColor
from PIL import ImageDraw
from PIL import ImageFont
from PIL import ImageOps

# For measuring the inference time.
import time

# Print Tensorflow version
print(tf.__version__)

# Check available GPU devices.
print("The following GPU devices are available: %s" % tf.test.gpu_device_name())

def display_image(image):
  fig = plt.figure(figsize=(20, 15))
  plt.grid(False)
  plt.imshow(image)


def download_and_resize_image(url, new_width=256, new_height=256,
                              display=False):
  _, filename = tempfile.mkstemp(suffix=".jpg")
  response = urlopen(url)
  image_data = response.read()
  image_data = BytesIO(image_data)
  pil_image = Image.open(image_data)
  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)
  pil_image_rgb = pil_image.convert("RGB")
  pil_image_rgb.save(filename, format="JPEG", quality=90)
  print("Image downloaded to %s." % filename)
  if display:
    display_image(pil_image)
  return filename


def draw_bounding_box_on_image(image,
                               ymin,
                               xmin,
                               ymax,
                               xmax,
                               color,
                               font,
                               thickness=4,
                               display_str_list=()):
  """Adds a bounding box to an image."""
  draw = ImageDraw.Draw(image)
  im_width, im_height = image.size
  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,
                                ymin * im_height, ymax * im_height)
  draw.line([(left, top), (left, bottom), (right, bottom), (right, top),
             (left, top)],
            width=thickness,
            fill=color)

  # If the total height of the display strings added to the top of the bounding
  # box exceeds the top of the image, stack the strings below the bounding box
  # instead of above.
  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]
  # Each display_str has a top and bottom margin of 0.05x.
  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)

  if top > total_display_str_height:
    text_bottom = top
  else:
    text_bottom = bottom + total_display_str_height
  # Reverse list and print from bottom to top.
  for display_str in display_str_list[::-1]:
    text_width, text_height = font.getsize(display_str)
    margin = np.ceil(0.05 * text_height)
    draw.rectangle([(left, text_bottom - text_height - 2 * margin),
                    (left + text_width, text_bottom)],
                   fill=color)
    draw.text((left + margin, text_bottom - text_height - margin),
              display_str,
              fill="black",
              font=font)
    text_bottom -= text_height - 2 * margin


def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):
  """Overlay labeled boxes on an image with formatted scores and label names."""
  colors = list(ImageColor.colormap.values())

  try:
    font = ImageFont.truetype("/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf",
                              25)
  except IOError:
    print("Font not found, using default font.")
    font = ImageFont.load_default()

  for i in range(min(boxes.shape[0], max_boxes)):
    if scores[i] >= min_score:
      ymin, xmin, ymax, xmax = tuple(boxes[i])
      display_str = "{}: {}%".format(class_names[i].decode("ascii"),
                                     int(100 * scores[i]))
      color = colors[hash(class_names[i]) % len(colors)]
      image_pil = Image.fromarray(np.uint8(image)).convert("RGB")
      draw_bounding_box_on_image(
          image_pil,
          ymin,
          xmin,
          ymax,
          xmax,
          color,
          font,
          display_str_list=[display_str])
      np.copyto(image, np.array(image_pil))
  return image

image_url =   "https://farm1.staticflickr.com/4032/4653948754_c0d768086b_o.jpg"#@param
downloaded_image_path = download_and_resize_image(image_url, 1280, 856, True)

module_handle = "https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1" #@param ["https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1", "https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1"]

detector = hub.load(module_handle).signatures['default']

def load_img(path):
  img = tf.io.read_file(path)
  img = tf.image.decode_jpeg(img, channels=3)
  return img

def run_detector(detector, path):
  img = load_img(path)

  converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]
  start_time = time.time()
  result = detector(converted_img)
  end_time = time.time()

  result = {key:value.numpy() for key,value in result.items()}

  print("Found %d objects." % len(result["detection_scores"]))
  print("Inference time: ", end_time-start_time)

  image_with_boxes = draw_boxes(
      img.numpy(), result["detection_boxes"],
      result["detection_class_entities"], result["detection_scores"])

  display_image(image_with_boxes)

run_detector(detector, downloaded_image_path)

image_urls = ["https://www.weekendnotes.com/im/001/09/public-place-chisholm-canberra-cafe-breakfast-lunc.jpg",
              "https://www.weekendnotes.com/im/007/01/jamaica-bay-wildlife-refuge-.jpg",
              "https://c4.staticflickr.com/9/8322/8053836633_6dc507f090_o.jpg"]

for image_url in image_urls:
  start_time = time.time()
  image_path = download_and_resize_image(image_url, 640, 480)
  run_detector(detector, image_path)
  end_time = time.time()
  print("Inference time:")